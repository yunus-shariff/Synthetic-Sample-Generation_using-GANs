{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1650976727196,"user":{"displayName":"Joe Despres","userId":"11492265170092125564"},"user_tz":240},"id":"hrpAj7PfN8d6","outputId":"5add34e5-aff2-42e6-82f7-10c1dcf7a147"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-67eb4670-ab89-4909-53d3-b0f0cbbb7086)\n"]}],"source":["!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHi2BE1wNG3x"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13290,"status":"ok","timestamp":1650976743535,"user":{"displayName":"Joe Despres","userId":"11492265170092125564"},"user_tz":240},"id":"vc-vcG2_X-L3","outputId":"502393cd-9c18-4180-8dca-7ff5e1548767"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["if 'google.colab' in str(get_ipython()):\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  proj_dir = \"/content/drive/MyDrive/ece884_project/\"\n","else:\n","  proj_dir = \"../\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QUsvJVMA96Lc"},"outputs":[],"source":["import os\n","import re\n","import pickle\n","models = os.listdir(f\"{proj_dir}saved_models/list_of_models/gen\")\n","model_number = [int(re.sub(\"generators\", \"\", x)) for x in models]\n","last_model = max(model_number)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NcA1lScbtDcF"},"outputs":[],"source":["df = pd.read_csv(f\"{proj_dir}data_clean/taxi.csv\")\n","column_names = df.columns\n","df = df.to_numpy()\n","from sklearn import preprocessing\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = preprocessing.MinMaxScaler().fit(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPO14qCxNZQY"},"outputs":[],"source":["# TODO lets put this away as a script\n","import tensorflow as tf\n","\n","import numpy as np\n","from tensorflow import keras\n","\n","def build_network(output_dim, n_hidden, n_neurons, learning_rate):\n","\n","    \"\"\"\n","\n","    output_dim: what do we want this to output\n","    Generator output n_columns of data\n","    Discriminator output 1, p(data_real|data_seen)\n","\n","    n_hiden: number of layers of the neural net\n","\n","    n_neurons: number of neuros in the network\n","\n","    learning_rate: duhhh\n","\n","    This outputs a keras neural net\n","    \n","    \"\"\"\n","    model = keras.models.Sequential()\n","    model.add(keras.layers.Flatten())\n","    for _ in range(n_hidden):\n","        model.add(keras.layers.Dense(n_neurons, activation=\"selu\"))\n","        # model.add(keras.layers.BatchNormalization())\n","    model.add(keras.layers.Dense(output_dim + 3, activation=\"selu\"))  \n","    model.add(keras.layers.Dense(output_dim, activation=\"sigmoid\"))\n","    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer)\n","    return model\n","\n","def train_gan(\n","    generator, discriminator, dataset, n_epochs=100, n_noise=20000\n","):\n","    \"\"\"\n","    # TODO: UPDATE ARGS\n","    Inputs: \n","\n","    gan, this is a keras gan object made by combining two neural nets and\n","    restricting the trainability of one of them.\n","\n","    dataset, this takes in regular tabular data. now this is training rowwise\n","    however i may change this to matrix wise like a picture.\n","\n","    n_epochs, numper of times the gans go though training iterationations\n","\n","    iterationations, number of times in gan iterationaton loop, \n","    it would be a good idea to reduct this after the warmup period\n","\n","    n_noise, this is the size of fake data generated\n","\n","    \n","    Output:\n","\n","    generators_saved, this is an iterationable list of keras objects that can be used\n","    \n","    discriminators_saved, same thing, these can be used to test\n","\n","    for generator, discriminator in zip(gen, desc):\n","        noise = tf.random.normal(shape=dims)\n","        generated_data = generator(noise)\n","        judgement = discriminator(generated_data) # probs data is real\n","    \"\"\"\n","    gan = keras.models.Sequential([generator, discriminator])\n","  \n","    discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n","    discriminator.trainable = False\n","    gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n","    generator, discriminator = gan.layers\n","    data_out = np.empty((0, dataset.shape[1]))\n","\n","\n","    for epoch in range(250):\n","        np.random.seed(epoch)\n","        random_index = tf.random.uniform(shape=(n_noise,), minval=0, maxval=len(dataset), dtype=tf.int32)\n","        X_batch = dataset[random_index, :]\n","        for iteration in range(5):\n","\n","            noise = tf.random.normal(shape=X_batch.shape,\n","                                     mean=0,\n","                                     stddev=1) \n","\n","            generated_data = generator(noise)\n","            X_fake_and_real = tf.concat([generated_data, X_batch], axis=0)\n","            y1 = tf.concat([tf.zeros(n_noise), tf.ones(n_noise)], axis=0)\n","            \n","            # training discriminator\n","            discriminator.trainable = True\n","            discriminator.train_on_batch(X_fake_and_real, y1)\n","            # training the generator\n","\n","            noise = tf.random.normal(shape=X_batch.shape,\n","                                     mean=0,\n","                                     stddev=1) \n","            \n","            discriminator.trainable = False\n","            gan.train_on_batch(noise, tf.ones(n_noise))\n","         \n","        generated_data = generator(noise)\n","        rand = tf.random.uniform(shape=(1,), minval=0, maxval=X_batch.shape[0], dtype=tf.int32)\n","\n","        data_out = np.concatenate([data_out, generated_data[ :1 , :]])\n","    \n","    return data_out\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"VIXppGoR3jkc"},"source":["lets consider initializing a new gan with each epoch or "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iVCVK6wdH8Y4"},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXIg0Hh3EQHk"},"outputs":[],"source":["neurons = np.random.randint(8, 25)\n","hidden = np.random.randint(0, 5)\n","noise_n = np.random.randint(50, 500)\n","epochs = np.random.randint(5, 100)\n","learn = np.random.exponential(1e-2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"_kPcJEnW1kZn","outputId":"d11a25d4-3723-4033-e3b2-9e507048521c"},"outputs":[{"name":"stderr","output_type":"stream","text":[" 20%|██        | 1020/5000 [7:36:03<29:51:30, 27.01s/it]"]}],"source":["for i in tqdm(range(5000)):\n","\n","    neurons = np.random.randint(8, 25)\n","    hidden = np.random.randint(0, 5)\n","    noise_n = np.random.randint(1, 25)\n","    epochs = np.random.randint(1, 100)\n","    learna = np.random.exponential(1e-2)\n","    learnb = np.random.exponential(1e-2)\n","\n","    generator = build_network(output_dim=df.shape[1], n_hidden=hidden, n_neurons=neurons, learning_rate=learna)\n","    discriminator = build_network(output_dim=1, n_hidden=hidden, n_neurons=neurons, learning_rate=learnb) \n","    \n","    gen_data = train_gan(generator, discriminator, df, n_epochs=epochs, n_noise=noise_n)\n","    output_path = f\"{proj_dir}data_generated/gan_gen_random_params_3.csv\"\n","    generated_data = pd.DataFrame(scaler.inverse_transform(gen_data), columns=column_names) \n","    generated_data.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTL71073q13E"},"outputs":[],"source":["\n","def generated_data_filter(gen, desc, points_to_gen, threashold, dims):\n","    \"\"\"\n","    inputs\n","    gen, is the list of gans we wrote with the gan.ipynb\n","\n","    desc, is the list of discriminators in the notebook gan.ipynb\n","    \n","    points_to_gen, number of datapoints for each model to generate\n","\n","    threashold, is what is the discriminator's predicted probability of the data being real\n","    we need to see to keep the data. \n","    with a threashold = 0.99 we will drop every datapoint that the discriminator says has a \n","    less than .99 change of being real. \n","    we will need to play with this.\n","\n","    \"\"\"\n","    n_col = dims[1]\n","    quality_data = np.empty((0, n_col), np.float32)\n","\n","    for generator, discriminator in zip(gen, desc):\n","        noise = tf.random.normal(shape=(points_to_gen, n_col))\n","        generated_data = generator(noise)\n","        judgement = discriminator(generated_data) # probs data is real\n","        data_fooling_discriminator = np.compress(np.ravel(judgement) > threashold, generated_data, axis=0)\n","\n","        quality_data = np.append(quality_data, data_fooling_discriminator, axis=0)\n","    \n","    for discriminator in desc:\n","        judgement = discriminator(quality_data)\n","        quality_data = np.compress(np.ravel(judgement) > threashold, quality_data, axis=0)\n","    return quality_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScapvoujsghA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"gan.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
